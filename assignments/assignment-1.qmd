---
title: "Assignment 1"
---
Ps. All the codes below has written with the combined power of AI & HI.
If you want to reach promts of codes that is used while preparing assignment, you can access the pdf document. 
ðŸ“„ [Download Prompts](prompts.pdf){.download target="_blank"}



My first assignment has three parts.

## (a)

I summarized the lesson video in which Mustafa BaydoÄŸan was the guest.
```{r, results="asis"}
knitr::asis_output('<iframe width="560" height="315" src="https://www.youtube.com/embed/1Mvkn71dhaA?si=t_wcZSM6ZRUQL9In" frameborder="0" allowfullscreen></iframe>')
```

Mustafa BaydoÄŸan (associate professor of Industrial Engineering at BOUN) explained the basic steps of data processing and analysis starting from a project he did during his PhD. The problem was defined as the deformation of timber after the drying process. Timbers with a tendency to deformation were analyzed with image processing method to determine. The importance of feature determination and the differences between traditional learning and deep learning were mentioned through this example. Then, the importance of production and consumption balance in the electricity market was explained. The high cost of imbalance and the damage caused by this were explained with mathematical values. The importance of the accuracy of demand estimates made for retail electricity companies was explained. Then, the subject of product recommendation of e-commerce sites was mentioned. It was explained that the most important factor here is digital footprint data and that the estimates produced based on this data are used to attract customer attention. It was discussed how Google trends data indicates internet sales in advance. The methods of determining various movements of product recommendation estimates determined by machine learning methods based on human behavior were explained. The importance of open data sources in estimation methods and the fact that human guidance and interpretation are still very important were mentioned. As new methods, the development and importance of point estimators and physics-based estimation methods that will bring a new dimension to estimations made solely on error minimization were mentioned. New developments on the use of machine learning methods in optimization were mentioned. In summary, estimation models and their impact and benefits on various sectors were mentioned throughout the course. In all sectors from woodworking to electricity market, from logistics systems to e-commerce; the importance of the financial success that data analysis brings to the sector was mentioned, as it facilitates process interpretation. In addition, the future points that data analysis processes, which are still developing, will reach with innovations were explained.


## (b.1)

```{r}
compute_stats <- function(data) {
  results <- list()
  
  for (col_name in colnames(data)) {
    column_values <- data[[col_name]]
    
    stats <- list(
      mean = mean(column_values, na.rm = TRUE),
      median = median(column_values, na.rm = TRUE),
      variance = var(column_values, na.rm = TRUE),
      IQR = IQR(column_values, na.rm = TRUE),
      min = min(column_values, na.rm = TRUE),
      max = max(column_values, na.rm = TRUE)
    )
    
    results[[col_name]] <- stats
  }
  
  return(results)
}

data(mtcars)

stats_results <- compute_stats(mtcars)

print(stats_results)

```

## (b.2)

```{r}
compute_stats_sapply <- function(data) {
  stats <- sapply(data, function(column) {
    list(
      mean = mean(column, na.rm = TRUE),
      median = median(column, na.rm = TRUE),
      variance = var(column, na.rm = TRUE),
      IQR = IQR(column, na.rm = TRUE),
      min = min(column, na.rm = TRUE),
      max = max(column, na.rm = TRUE)
    )
  }, simplify = FALSE)  
  
  return(stats)
}

data(mtcars)

stats_results_sapply <- compute_stats_sapply(mtcars)

print(stats_results_sapply)

```

## (b.3)

```{r}
compute_stats_apply <- function(data) {
  stats_matrix <- apply(data, 2, function(column) {
    c(
      mean = mean(column, na.rm = TRUE),
      median = median(column, na.rm = TRUE),
      variance = var(column, na.rm = TRUE),
      IQR = IQR(column, na.rm = TRUE),
      min = min(column, na.rm = TRUE),
      max = max(column, na.rm = TRUE)
    )
  })
  
  return(as.data.frame(stats_matrix))  
}

stats_results_apply <- compute_stats_apply(as.matrix(mtcars))

print(stats_results_apply)

```

## (c)

```{r}
library(dslabs)
data("na_example")
na_example
```

```{r}

na_count <- sum(is.na(na_example))

print("Here, you can see the number of NA in this dataset:")
print(na_count)

```

```{r}

na_indexes <- which(is.na(na_example))

print("Here, you can basicly see the indexes of NA data:")
print(na_indexes)

```

```{r}

library(dslabs)
data("na_example")

mean_value <- mean(na_example, na.rm = TRUE)
sd_value <- sd(na_example, na.rm = TRUE)

print(paste("Mean:", mean_value))
print(paste("Standard Deviation:", sd_value))


```

```{r}

library(dslabs)
data("na_example")

mape <- function(actual, predicted) {
  return(mean(abs((actual - predicted) / actual)) * 100)
}

library(dslabs)
data("na_example")

median_value <- median(na_example, na.rm = TRUE)
version1 <- na_example
version1[is.na(version1)] <- median_value

set.seed(42)
non_na_values <- na_example[!is.na(na_example)]
version2 <- na_example
version2[is.na(version2)] <- sample(non_na_values, sum(is.na(version2)), replace = TRUE)

means <- c(mean(na_example, na.rm = TRUE),
           mean(version1, na.rm = TRUE),
           mean(version2, na.rm = TRUE))

sds <- c(sd(na_example, na.rm = TRUE),
         sd(version1, na.rm = TRUE),
         sd(version2, na.rm = TRUE))

mape_means_v1 <- mape(means[1], means[2])
mape_means_v2 <- mape(means[1], means[3])
mape_sds_v1 <- mape(sds[1], sds[2])
mape_sds_v2 <- mape(sds[1], sds[3])

stats_df <- data.frame(
  Version = c("Original", "Median Imputed", "Random Imputed"),
  Mean = means,
  Standard_Deviation = sds,
  MAPE_Mean = c(NA, mape_means_v1, mape_means_v2),  # MAPE for mean
  MAPE_SD = c(NA, mape_sds_v1, mape_sds_v2)         # MAPE for standard deviation
)

print(stats_df)




```

As seen on the table the version 2 array's mean and SD values are closer to the original dataset. It means that, to obtain a new dataset with the characteristic of original one, changing NA values with the randomly selected data from original dataset is more relevant.
